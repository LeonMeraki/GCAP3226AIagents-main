{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b644a8ac",
   "metadata": {},
   "source": [
    "# Web Crawling for Policy Analysis\n",
    "## GCAP3226: Empowering Citizens through Data\n",
    "\n",
    "**Learning Objectives:**\n",
    "1. Understand ethical web crawling practices\n",
    "2. Analyze robots.txt and sitemaps\n",
    "3. Identify pages with quantitative data\n",
    "4. Extract and organize data from government websites\n",
    "5. Apply these techniques to policy research\n",
    "\n",
    "**Case Study:** Cyberdefender.hk - Hong Kong Government Cybersecurity Portal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2067ed4d",
   "metadata": {},
   "source": [
    "## Part 1: Understanding Web Crawling Ethics\n",
    "\n",
    "### What is robots.txt?\n",
    "The `robots.txt` file tells web crawlers which parts of a website they can access. It's a fundamental part of web crawling ethics.\n",
    "\n",
    "**Key Rules:**\n",
    "- Always check robots.txt before crawling\n",
    "- Respect the directives (Disallow, Allow)\n",
    "- Implement rate limiting to avoid overloading servers\n",
    "- Identify your crawler with a descriptive User-Agent\n",
    "\n",
    "Let's check cyberdefender.hk's robots.txt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17d104c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "robots.txt content:\n",
      "======================================================================\n",
      "# START YOAST BLOCK\n",
      "# ---------------------------\n",
      "User-agent: *\n",
      "Disallow:\n",
      "\n",
      "Sitemap: https://cyberdefender.hk/sitemap_index.xml\n",
      "# ---------------------------\n",
      "# END YOAST BLOCK\n",
      "======================================================================\n",
      "\n",
      "Summary for User-agent: *\n",
      "  Disallow rules: 0\n",
      "  Allow rules: 0\n",
      "  Crawl-delay: not specified\n",
      "\n",
      "Sitemaps found:\n",
      "  - https://cyberdefender.hk/sitemap_index.xml\n",
      "\n",
      "✓ Analysis: No Disallow directives for User-agent: *. Crawling appears permitted, subject to site terms and ethical rate limiting.\n",
      "Recommended delay between requests: 1.0 seconds\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import xml.etree.ElementTree as ET\n",
    "import re\n",
    "import pandas as pd\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "# Target website\n",
    "BASE_URL = \"https://cyberdefender.hk\"\n",
    "\n",
    "# Prepare a session with a polite User-Agent\n",
    "session = requests.Session()\n",
    "session.headers.update({\n",
    "    \"User-Agent\": \"GCAP3226-Student-Crawler/1.0 (Educational purpose)\"\n",
    "})\n",
    "\n",
    "# Fetch and analyze robots.txt\n",
    "robots_url = f\"{BASE_URL}/robots.txt\"\n",
    "print(\"robots.txt content:\")\n",
    "print(\"=\" * 70)\n",
    "try:\n",
    "    response = session.get(robots_url, timeout=10)\n",
    "    response.raise_for_status()\n",
    "    text = response.text\n",
    "    print(text)\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # Parse robots.txt (simple parser focused on essentials)\n",
    "    rules = {\n",
    "        \"user_agents\": {},\n",
    "        \"sitemaps\": []\n",
    "    }\n",
    "    current_ua = None\n",
    "\n",
    "    for raw in text.splitlines():\n",
    "        line = raw.strip()\n",
    "        if not line or line.startswith('#'):\n",
    "            continue\n",
    "        key, _, value = line.partition(':')\n",
    "        key = key.strip().lower()\n",
    "        value = value.strip()\n",
    "\n",
    "        if key == 'user-agent':\n",
    "            current_ua = value\n",
    "            rules[\"user_agents\"].setdefault(current_ua, {\"allow\": [], \"disallow\": [], \"crawl-delay\": None})\n",
    "        elif key in (\"allow\", \"disallow\"):\n",
    "            if current_ua is None:\n",
    "                continue\n",
    "            rules[\"user_agents\"].setdefault(current_ua, {\"allow\": [], \"disallow\": [], \"crawl-delay\": None})\n",
    "            rules[\"user_agents\"][current_ua][key].append(value)\n",
    "        elif key == \"crawl-delay\":\n",
    "            if current_ua is None:\n",
    "                continue\n",
    "            try:\n",
    "                delay_val = float(value)\n",
    "            except ValueError:\n",
    "                delay_val = value  # keep as-is if not numeric\n",
    "            rules[\"user_agents\"][current_ua][\"crawl-delay\"] = delay_val\n",
    "        elif key == 'sitemap':\n",
    "            rules[\"sitemaps\"].append(value)\n",
    "\n",
    "    # Summarize for generic user agent '*'\n",
    "    ua_all = rules[\"user_agents\"].get('*')\n",
    "    if ua_all:\n",
    "        disallows = [p for p in ua_all['disallow'] if p is not None and p.strip() != '']\n",
    "        allows = [p for p in ua_all['allow'] if p is not None and p.strip() != '']\n",
    "        print(\"\\nSummary for User-agent: *\")\n",
    "        print(f\"  Disallow rules: {len(disallows)}\")\n",
    "        print(f\"  Allow rules: {len(allows)}\")\n",
    "        print(f\"  Crawl-delay: {ua_all['crawl-delay'] if ua_all['crawl-delay'] is not None else 'not specified'}\")\n",
    "    else:\n",
    "        print(\"\\nNo explicit section for User-agent: * found in robots.txt\")\n",
    "\n",
    "    # List sitemaps if present\n",
    "    if rules[\"sitemaps\"]:\n",
    "        print(\"\\nSitemaps found:\")\n",
    "        for sm in rules[\"sitemaps\"][:10]:\n",
    "            print(f\"  - {sm}\")\n",
    "        if len(rules[\"sitemaps\"]) > 10:\n",
    "            print(f\"  ... (+{len(rules['sitemaps']) - 10} more)\")\n",
    "\n",
    "    # Analysis message (non-committal, based on Disallow count)\n",
    "    if ua_all and len([p for p in ua_all['disallow'] if p.strip() != '']) == 0:\n",
    "        print(\"\\n✓ Analysis: No Disallow directives for User-agent: *. Crawling appears permitted, subject to site terms and ethical rate limiting.\")\n",
    "    elif ua_all:\n",
    "        print(f\"\\n⚠ Analysis: Found {len([p for p in ua_all['disallow'] if p.strip() != ''])} Disallow rule(s) for User-agent: *. Respect these paths.\")\n",
    "    else:\n",
    "        print(\"\\nℹ Analysis: robots.txt does not specify rules for User-agent: *. Proceed cautiously and respect site terms.\")\n",
    "\n",
    "    # Recommend a conservative delay\n",
    "    recommended_delay = 1.0\n",
    "    if ua_all and isinstance(ua_all.get('crawl-delay'), (int, float)):\n",
    "        recommended_delay = max(1.0, float(ua_all['crawl-delay']))\n",
    "    print(f\"Recommended delay between requests: {recommended_delay} seconds\")\n",
    "\n",
    "except requests.RequestException as e:\n",
    "    print(\"Error fetching robots.txt:\", e)\n",
    "    print(\"=\" * 70)\n",
    "    print(\"ℹ Tip: Check your internet connection or try again later.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c615736",
   "metadata": {},
   "source": [
    "### Exercise 1.1: Check robots.txt for other government websites\n",
    "\n",
    "Try checking robots.txt for these Hong Kong government sites:\n",
    "- https://www.info.gov.hk\n",
    "- https://www.censtatd.gov.hk (Census and Statistics Department)\n",
    "- https://www.epd.gov.hk (Environmental Protection Department)\n",
    "\n",
    "**Question:** Do they all allow crawling? Are there any restrictions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b285f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Robots.txt summary for HK government sites:\n",
      "\n",
      "Site: https://www.info.gov.hk\n",
      "  User-agent '*': present\n",
      "  Disallow rules: 1\n",
      "  Allow rules: 0\n",
      "  Crawl-delay: not specified\n",
      "  ⚠ Restrictions present — respect Disallow paths.\n",
      "\n",
      "Site: https://www.censtatd.gov.hk\n",
      "  User-agent '*': present\n",
      "  Disallow rules: 1\n",
      "  Allow rules: 0\n",
      "  Crawl-delay: not specified\n",
      "  ⚠ Restrictions present — respect Disallow paths.\n",
      "\n",
      "Site: https://www.epd.gov.hk\n",
      "  ✗ Error: 404 Client Error: Not Found for url: https://www.epd.gov.hk/robots.txt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exercise 1.1 solution: fetch and summarize robots.txt for multiple HK government sites\n",
    "import requests\n",
    "from typing import Dict, Any\n",
    "\n",
    "sites = [\n",
    "    \"https://www.info.gov.hk\",\n",
    "    \"https://www.censtatd.gov.hk\",\n",
    "    \"https://www.epd.gov.hk\",\n",
    "]\n",
    "\n",
    "session = requests.Session()\n",
    "session.headers.update({\n",
    "    \"User-Agent\": \"GCAP3226-Student-Crawler/1.0 (Educational purpose)\"\n",
    "})\n",
    "\n",
    "\n",
    "def summarize_robots(base_url: str) -> Dict[str, Any]:\n",
    "    url = base_url.rstrip('/') + \"/robots.txt\"\n",
    "    out: Dict[str, Any] = {\n",
    "        \"site\": base_url,\n",
    "        \"status\": \"ok\",\n",
    "        \"ua_star\": None,\n",
    "        \"disallow_count\": None,\n",
    "        \"allow_count\": None,\n",
    "        \"crawl_delay\": None,\n",
    "        \"sitemaps\": [],\n",
    "        \"note\": \"\",\n",
    "    }\n",
    "    try:\n",
    "        r = session.get(url, timeout=12)\n",
    "        r.raise_for_status()\n",
    "        text = r.text\n",
    "        rules = {\"user_agents\": {}, \"sitemaps\": []}\n",
    "        current_ua = None\n",
    "        for raw in text.splitlines():\n",
    "            line = raw.strip()\n",
    "            if not line or line.startswith('#'):\n",
    "                continue\n",
    "            key, _, value = line.partition(':')\n",
    "            key = key.strip().lower()\n",
    "            value = value.strip()\n",
    "            if key == 'user-agent':\n",
    "                current_ua = value\n",
    "                rules[\"user_agents\"].setdefault(current_ua, {\"allow\": [], \"disallow\": [], \"crawl-delay\": None})\n",
    "            elif key in (\"allow\", \"disallow\"):\n",
    "                if current_ua is None:\n",
    "                    continue\n",
    "                rules[\"user_agents\"].setdefault(current_ua, {\"allow\": [], \"disallow\": [], \"crawl-delay\": None})\n",
    "                rules[\"user_agents\"][current_ua][key].append(value)\n",
    "            elif key == 'crawl-delay':\n",
    "                if current_ua is None:\n",
    "                    continue\n",
    "                try:\n",
    "                    delay_val = float(value)\n",
    "                except ValueError:\n",
    "                    delay_val = value\n",
    "                rules[\"user_agents\"][current_ua][\"crawl-delay\"] = delay_val\n",
    "            elif key == 'sitemap':\n",
    "                rules[\"sitemaps\"].append(value)\n",
    "        ua_all = rules[\"user_agents\"].get('*')\n",
    "        out[\"ua_star\"] = bool(ua_all)\n",
    "        if ua_all:\n",
    "            out[\"disallow_count\"] = len([p for p in ua_all['disallow'] if p and p.strip()])\n",
    "            out[\"allow_count\"] = len([p for p in ua_all['allow'] if p and p.strip()])\n",
    "            out[\"crawl_delay\"] = ua_all.get('crawl-delay')\n",
    "        out[\"sitemaps\"] = rules[\"sitemaps\"]\n",
    "    except requests.RequestException as e:\n",
    "        out[\"status\"] = \"error\"\n",
    "        out[\"note\"] = str(e)\n",
    "    return out\n",
    "\n",
    "\n",
    "results = [summarize_robots(s) for s in sites]\n",
    "\n",
    "print(\"Robots.txt summary for HK government sites:\\n\")\n",
    "for res in results:\n",
    "    print(f\"Site: {res['site']}\")\n",
    "    if res[\"status\"] != \"ok\":\n",
    "        print(f\"  ✗ Error: {res['note']}\")\n",
    "        print()\n",
    "        continue\n",
    "    if res[\"ua_star\"]:\n",
    "        print(\"  User-agent '*': present\")\n",
    "        print(f\"  Disallow rules: {res['disallow_count']}\")\n",
    "        print(f\"  Allow rules: {res['allow_count']}\")\n",
    "        print(f\"  Crawl-delay: {res['crawl_delay'] if res['crawl_delay'] is not None else 'not specified'}\")\n",
    "        if (res['disallow_count'] or 0) == 0:\n",
    "            print(\"  ✓ Crawling generally permitted for '*', subject to terms.\")\n",
    "        else:\n",
    "            print(\"  ⚠ Restrictions present — respect Disallow paths.\")\n",
    "    else:\n",
    "        print(\"  ℹ No explicit rules for User-agent '*'. Proceed cautiously.\")\n",
    "    if res[\"sitemaps\"]:\n",
    "        print(\"  Sitemaps:\")\n",
    "        for sm in res[\"sitemaps\"][:5]:\n",
    "            print(f\"    - {sm}\")\n",
    "        if len(res[\"sitemaps\"]) > 5:\n",
    "            print(f\"    ... (+{len(res['sitemaps'])-5} more)\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edf586e",
   "metadata": {},
   "source": [
    "## Part 2: Discovering Content with Sitemaps\n",
    "\n",
    "Sitemaps are XML files that list all pages on a website. They make crawling much more efficient!\n",
    "\n",
    "### Sitemap Index\n",
    "Large websites often have a `sitemap_index.xml` that points to multiple sitemaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "372083ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 sitemaps:\n",
      "\n",
      "                                               url             last_modified                      name\n",
      "         https://cyberdefender.hk/post-sitemap.xml 2025-04-07T15:28:23+00:00          post-sitemap.xml\n",
      "         https://cyberdefender.hk/page-sitemap.xml 2025-10-14T03:20:38+00:00          page-sitemap.xml\n",
      "https://cyberdefender.hk/gm_menu_block-sitemap.xml 2021-06-21T09:48:00+00:00 gm_menu_block-sitemap.xml\n",
      "          https://cyberdefender.hk/r3d-sitemap.xml 2022-06-27T05:26:00+00:00           r3d-sitemap.xml\n",
      "https://cyberdefender.hk/sdm_downloads-sitemap.xml 2025-10-09T02:33:07+00:00 sdm_downloads-sitemap.xml\n",
      "     https://cyberdefender.hk/category-sitemap.xml 2025-10-14T03:20:38+00:00      category-sitemap.xml\n"
     ]
    }
   ],
   "source": [
    "# Fetch sitemap index\n",
    "sitemap_index_url = f\"{BASE_URL}/sitemap_index.xml\"\n",
    "response = requests.get(sitemap_index_url)\n",
    "\n",
    "# Parse XML\n",
    "root = ET.fromstring(response.content)\n",
    "namespace = {'ns': 'http://www.sitemaps.org/schemas/sitemap/0.9'}\n",
    "\n",
    "# Extract sitemap URLs\n",
    "sitemaps = []\n",
    "for sitemap in root.findall('.//ns:sitemap', namespace):\n",
    "    loc = sitemap.find('ns:loc', namespace)\n",
    "    lastmod = sitemap.find('ns:lastmod', namespace)\n",
    "    if loc is not None:\n",
    "        sitemaps.append({\n",
    "            'url': loc.text,\n",
    "            'last_modified': lastmod.text if lastmod is not None else 'N/A',\n",
    "            'name': loc.text.split('/')[-1]\n",
    "        })\n",
    "\n",
    "# Display as DataFrame\n",
    "df_sitemaps = pd.DataFrame(sitemaps)\n",
    "print(f\"Found {len(sitemaps)} sitemaps:\\n\")\n",
    "print(df_sitemaps.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70618b63",
   "metadata": {},
   "source": [
    "### Key Observation\n",
    "Notice the `sdm_downloads-sitemap.xml` - this contains downloadable files, which often include quantitative data like reports, statistics, and datasets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b29c066f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "post-sitemap.xml: 6 URLs\n",
      "page-sitemap.xml: 102 URLs\n",
      "page-sitemap.xml: 102 URLs\n",
      "gm_menu_block-sitemap.xml: 1 URLs\n",
      "gm_menu_block-sitemap.xml: 1 URLs\n",
      "r3d-sitemap.xml: 4 URLs\n",
      "r3d-sitemap.xml: 4 URLs\n",
      "sdm_downloads-sitemap.xml: 21 URLs\n",
      "sdm_downloads-sitemap.xml: 21 URLs\n",
      "category-sitemap.xml: 15 URLs\n",
      "\n",
      "Total URLs to crawl: 149\n",
      "category-sitemap.xml: 15 URLs\n",
      "\n",
      "Total URLs to crawl: 149\n"
     ]
    }
   ],
   "source": [
    "# Function to fetch and parse a sitemap\n",
    "def fetch_sitemap(sitemap_url):\n",
    "    \"\"\"Fetch and parse a sitemap XML file\"\"\"\n",
    "    response = requests.get(sitemap_url)\n",
    "    root = ET.fromstring(response.content)\n",
    "    namespace = {'ns': 'http://www.sitemaps.org/schemas/sitemap/0.9'}\n",
    "    \n",
    "    urls = []\n",
    "    for url_elem in root.findall('.//ns:url', namespace):\n",
    "        loc = url_elem.find('ns:loc', namespace)\n",
    "        if loc is not None:\n",
    "            urls.append(loc.text)\n",
    "    \n",
    "    return urls\n",
    "\n",
    "# Fetch all URLs from all sitemaps\n",
    "all_urls = {}\n",
    "for sitemap in sitemaps:\n",
    "    time.sleep(1)  # Rate limiting\n",
    "    urls = fetch_sitemap(sitemap['url'])\n",
    "    all_urls[sitemap['name']] = urls\n",
    "    print(f\"{sitemap['name']}: {len(urls)} URLs\")\n",
    "\n",
    "total_urls = sum(len(urls) for urls in all_urls.values())\n",
    "print(f\"\\nTotal URLs to crawl: {total_urls}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8c1a8b",
   "metadata": {},
   "source": [
    "## Part 3: Detecting Quantitative Data\n",
    "\n",
    "Not all pages have useful quantitative data. We need to be selective!\n",
    "\n",
    "### Detection Criteria:\n",
    "1. **Numbers:** Percentages, currency, statistics\n",
    "2. **Tables:** Structured data\n",
    "3. **Keywords:** \"statistics\", \"data\", \"analysis\", \"report\", \"survey\"\n",
    "4. **Charts/Graphs:** Visual data representations\n",
    "5. **Downloadable Files:** PDFs, Excel files, CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cdcf2b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Quantitative data detection function defined\n"
     ]
    }
   ],
   "source": [
    "def detect_quantitative_data(html_content, url):\n",
    "    \"\"\"\n",
    "    Analyze HTML content to detect quantitative data\n",
    "    Returns: (has_data, metadata)\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    text = soup.get_text()\n",
    "    \n",
    "    metadata = {\n",
    "        'url': url,\n",
    "        'has_numbers': False,\n",
    "        'has_tables': False,\n",
    "        'has_charts': False,\n",
    "        'number_count': 0,\n",
    "        'table_count': 0,\n",
    "        'keywords': []\n",
    "    }\n",
    "    \n",
    "    # 1. Count meaningful numbers\n",
    "    number_patterns = [\n",
    "        r'\\d+(?:\\.\\d+)?%',  # Percentages\n",
    "        r'\\$\\d+(?:,\\d{3})*',  # Currency\n",
    "        r'HK\\$\\d+(?:,\\d{3})*',  # HK currency\n",
    "        r'\\d{1,3}(?:,\\d{3})+',  # Numbers with commas\n",
    "    ]\n",
    "    \n",
    "    for pattern in number_patterns:\n",
    "        matches = re.findall(pattern, text)\n",
    "        metadata['number_count'] += len(matches)\n",
    "    \n",
    "    if metadata['number_count'] > 5:\n",
    "        metadata['has_numbers'] = True\n",
    "    \n",
    "    # 2. Check for tables\n",
    "    tables = soup.find_all('table')\n",
    "    metadata['table_count'] = len(tables)\n",
    "    metadata['has_tables'] = len(tables) > 0\n",
    "    \n",
    "    # 3. Check for keywords\n",
    "    keywords = ['statistics', 'data', 'analysis', 'report', 'survey',\n",
    "                'percentage', 'total', 'average', 'rate']\n",
    "    \n",
    "    for keyword in keywords:\n",
    "        if keyword.lower() in text.lower():\n",
    "            metadata['keywords'].append(keyword)\n",
    "    \n",
    "    # 4. Check for charts\n",
    "    chart_indicators = ['chart', 'graph', 'diagram']\n",
    "    for indicator in chart_indicators:\n",
    "        if indicator.lower() in text.lower():\n",
    "            metadata['has_charts'] = True\n",
    "            break\n",
    "    \n",
    "    # Determine if page has quantitative data\n",
    "    has_data = (\n",
    "        metadata['has_numbers'] or \n",
    "        metadata['has_tables'] or\n",
    "        (metadata['has_charts'] and len(metadata['keywords']) > 0)\n",
    "    )\n",
    "    \n",
    "    return has_data, metadata\n",
    "\n",
    "print(\"✓ Quantitative data detection function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1912a663",
   "metadata": {},
   "source": [
    "### Example: Test the Detection Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd816e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing URL: https://cyberdefender.hk/%e3%80%8c%e8%b5%b7%e5%ba%95%e3%80%8d%e8%a1%8c%e7%82%ba%e5%88%91%e4%ba%8b%e5%8c%96%e2%9d%97%ef%b8%8f/\n",
      "\n",
      "Has quantitative data: True\n",
      "\n",
      "Detection details:\n",
      "  Numbers found: 0\n",
      "  Tables found: 1\n",
      "  Has charts: False\n",
      "  Keywords: data, rate\n",
      "Has quantitative data: True\n",
      "\n",
      "Detection details:\n",
      "  Numbers found: 0\n",
      "  Tables found: 1\n",
      "  Has charts: False\n",
      "  Keywords: data, rate\n"
     ]
    }
   ],
   "source": [
    "# Test on a sample page\n",
    "test_url = all_urls['post-sitemap.xml'][0] if 'post-sitemap.xml' in all_urls else BASE_URL\n",
    "\n",
    "print(f\"Testing URL: {test_url}\\n\")\n",
    "response = requests.get(test_url)\n",
    "has_data, metadata = detect_quantitative_data(response.content, test_url)\n",
    "\n",
    "print(f\"Has quantitative data: {has_data}\")\n",
    "print(f\"\\nDetection details:\")\n",
    "print(f\"  Numbers found: {metadata['number_count']}\")\n",
    "print(f\"  Tables found: {metadata['table_count']}\")\n",
    "print(f\"  Has charts: {metadata['has_charts']}\")\n",
    "print(f\"  Keywords: {', '.join(metadata['keywords'][:5])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fa087c",
   "metadata": {},
   "source": [
    "## Part 4: Implementing Ethical Crawling\n",
    "\n",
    "### Best Practices:\n",
    "1. **Rate Limiting:** Wait between requests (1-2 seconds)\n",
    "2. **User-Agent:** Identify your crawler\n",
    "3. **Error Handling:** Handle 404s, timeouts gracefully\n",
    "4. **Logging:** Track what you're doing\n",
    "5. **Respect robots.txt:** Always follow the rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78c429f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Ethical crawler function defined\n"
     ]
    }
   ],
   "source": [
    "def crawl_with_ethics(urls, max_pages=10):\n",
    "    \"\"\"\n",
    "    Ethically crawl a list of URLs\n",
    "    \"\"\"\n",
    "    session = requests.Session()\n",
    "    session.headers.update({\n",
    "        'User-Agent': 'GCAP3226-Student-Crawler/1.0 (Educational Purpose)'\n",
    "    })\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i, url in enumerate(urls[:max_pages], 1):\n",
    "        print(f\"[{i}/{min(len(urls), max_pages)}] Crawling: {url}\")\n",
    "        \n",
    "        try:\n",
    "            response = session.get(url, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            has_data, metadata = detect_quantitative_data(response.content, url)\n",
    "            results.append({\n",
    "                'url': url,\n",
    "                'has_data': has_data,\n",
    "                'numbers': metadata['number_count'],\n",
    "                'tables': metadata['table_count'],\n",
    "                'keywords': ', '.join(metadata['keywords'][:3])\n",
    "            })\n",
    "            \n",
    "            if has_data:\n",
    "                print(f\"  ✓ Found quantitative data!\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Error: {str(e)}\")\n",
    "            results.append({\n",
    "                'url': url,\n",
    "                'has_data': False,\n",
    "                'numbers': 0,\n",
    "                'tables': 0,\n",
    "                'keywords': f'Error: {str(e)}'\n",
    "            })\n",
    "        \n",
    "        # Rate limiting - wait 2 seconds between requests\n",
    "        time.sleep(2)\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "print(\"✓ Ethical crawler function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a072af",
   "metadata": {},
   "source": [
    "### Exercise 4.1: Crawl Sample Pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ff9f200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling 5 sample pages...\n",
      "\n",
      "[1/5] Crawling: https://cyberdefender.hk/%e3%80%8c%e8%b5%b7%e5%ba%95%e3%80%8d%e8%a1%8c%e7%82%ba%e5%88%91%e4%ba%8b%e5%8c%96%e2%9d%97%ef%b8%8f/\n",
      "  ✓ Found quantitative data!\n",
      "  ✓ Found quantitative data!\n",
      "[2/5] Crawling: https://cyberdefender.hk/whatsapp-scam/\n",
      "[2/5] Crawling: https://cyberdefender.hk/whatsapp-scam/\n",
      "  ✓ Found quantitative data!\n",
      "  ✓ Found quantitative data!\n",
      "[3/5] Crawling: https://cyberdefender.hk/sms-scam/\n",
      "[3/5] Crawling: https://cyberdefender.hk/sms-scam/\n",
      "  ✓ Found quantitative data!\n",
      "  ✓ Found quantitative data!\n",
      "[4/5] Crawling: https://cyberdefender.hk/2024-dragon-year-fortune/\n",
      "[4/5] Crawling: https://cyberdefender.hk/2024-dragon-year-fortune/\n",
      "  ✗ Error: 404 Client Error: Not Found for url: https://cyberdefender.hk/2024-dragon-year-fortune/\n",
      "  ✗ Error: 404 Client Error: Not Found for url: https://cyberdefender.hk/2024-dragon-year-fortune/\n",
      "[5/5] Crawling: https://cyberdefender.hk/carousell-scam/\n",
      "[5/5] Crawling: https://cyberdefender.hk/carousell-scam/\n",
      "  ✓ Found quantitative data!\n",
      "  ✓ Found quantitative data!\n",
      "\n",
      "================================================================================\n",
      "RESULTS\n",
      "================================================================================\n",
      "                                                                                                                          url  has_data  numbers  tables                                                                                       keywords\n",
      "https://cyberdefender.hk/%e3%80%8c%e8%b5%b7%e5%ba%95%e3%80%8d%e8%a1%8c%e7%82%ba%e5%88%91%e4%ba%8b%e5%8c%96%e2%9d%97%ef%b8%8f/      True        0       1                                                                                     data, rate\n",
      "                                                                                      https://cyberdefender.hk/whatsapp-scam/      True        0       1                                                                                     data, rate\n",
      "                                                                                           https://cyberdefender.hk/sms-scam/      True        0       1                                                                                     data, rate\n",
      "                                                                           https://cyberdefender.hk/2024-dragon-year-fortune/     False        0       0 Error: 404 Client Error: Not Found for url: https://cyberdefender.hk/2024-dragon-year-fortune/\n",
      "                                                                                     https://cyberdefender.hk/carousell-scam/      True        0       1                                                                                     data, rate\n",
      "\n",
      "Pages with data: 4 / 5\n",
      "Success rate: 80.0%\n",
      "\n",
      "================================================================================\n",
      "RESULTS\n",
      "================================================================================\n",
      "                                                                                                                          url  has_data  numbers  tables                                                                                       keywords\n",
      "https://cyberdefender.hk/%e3%80%8c%e8%b5%b7%e5%ba%95%e3%80%8d%e8%a1%8c%e7%82%ba%e5%88%91%e4%ba%8b%e5%8c%96%e2%9d%97%ef%b8%8f/      True        0       1                                                                                     data, rate\n",
      "                                                                                      https://cyberdefender.hk/whatsapp-scam/      True        0       1                                                                                     data, rate\n",
      "                                                                                           https://cyberdefender.hk/sms-scam/      True        0       1                                                                                     data, rate\n",
      "                                                                           https://cyberdefender.hk/2024-dragon-year-fortune/     False        0       0 Error: 404 Client Error: Not Found for url: https://cyberdefender.hk/2024-dragon-year-fortune/\n",
      "                                                                                     https://cyberdefender.hk/carousell-scam/      True        0       1                                                                                     data, rate\n",
      "\n",
      "Pages with data: 4 / 5\n",
      "Success rate: 80.0%\n"
     ]
    }
   ],
   "source": [
    "# Crawl first 5 pages from the posts sitemap\n",
    "sample_urls = all_urls.get('post-sitemap.xml', [])[:5]\n",
    "\n",
    "print(f\"Crawling {len(sample_urls)} sample pages...\\n\")\n",
    "results_df = crawl_with_ethics(sample_urls, max_pages=5)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\nPages with data: {results_df['has_data'].sum()} / {len(results_df)}\")\n",
    "print(f\"Success rate: {results_df['has_data'].sum() / len(results_df) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafb3fe5",
   "metadata": {},
   "source": [
    "## Part 5: Comparing with Google Site-Specific Search\n",
    "\n",
    "Let's compare our crawler with Google's site-specific search.\n",
    "\n",
    "### Google Site Search Syntax:\n",
    "```\n",
    "site:cyberdefender.hk \"statistics\" OR \"data\" OR \"report\"\n",
    "```\n",
    "\n",
    "### Advantages of Crawler:\n",
    "1. **Systematic:** Covers all pages via sitemap\n",
    "2. **Programmable:** Automated data extraction\n",
    "3. **Customizable:** Your own detection criteria\n",
    "4. **Downloadable:** Save pages and files locally\n",
    "5. **Reproducible:** Can rerun with same parameters\n",
    "\n",
    "### Advantages of Google Search:\n",
    "1. **Fast:** Instant results\n",
    "2. **Smart:** Better keyword matching\n",
    "3. **No coding:** User-friendly interface\n",
    "4. **No restrictions:** Google handles rate limiting\n",
    "\n",
    "**Conclusion:** Use both! Google for quick discovery, crawlers for systematic data collection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd46b6f8",
   "metadata": {},
   "source": [
    "## Part 6: Your Assignment - Crawl a Government Website\n",
    "\n",
    "Choose one of these Hong Kong government websites and apply what you've learned:\n",
    "\n",
    "1. **Census and Statistics Department:** https://www.censtatd.gov.hk\n",
    "   - Rich in quantitative data\n",
    "   - Population, economy, social indicators\n",
    "\n",
    "2. **Environmental Protection Department:** https://www.epd.gov.hk\n",
    "   - Air quality data\n",
    "   - Waste statistics\n",
    "   - Environmental reports\n",
    "\n",
    "3. **Transport Department:** https://www.td.gov.hk\n",
    "   - Traffic statistics\n",
    "   - Public transport data\n",
    "   - Road safety reports\n",
    "\n",
    "4. **Food and Health Bureau:** https://www.fhb.gov.hk\n",
    "   - Healthcare statistics\n",
    "   - Disease surveillance data\n",
    "   - Hospital data\n",
    "\n",
    "### Assignment Tasks:\n",
    "\n",
    "1. **Check robots.txt** - Are you allowed to crawl?\n",
    "2. **Find sitemap** - Does the site have a sitemap?\n",
    "3. **Identify target pages** - What pages likely have quantitative data?\n",
    "4. **Crawl ethically** - Implement rate limiting and proper User-Agent\n",
    "5. **Extract data** - Download pages and files with quantitative data\n",
    "6. **Analyze results** - What did you find? What data is useful for policy analysis?\n",
    "7. **Document** - Write a brief report on your findings\n",
    "\n",
    "### Deliverables:\n",
    "1. Python code (use the functions from this notebook)\n",
    "2. Data files collected\n",
    "3. Summary report (500-1000 words)\n",
    "4. Reflection on ethical considerations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "512c26da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Robots.txt summary for FHB:\n",
      "{\n",
      "  \"robots_url\": \"https://www.fhb.gov.hk/robots.txt\",\n",
      "  \"ua_star_present\": false,\n",
      "  \"disallow_count\": 0,\n",
      "  \"allow_count\": 0,\n",
      "  \"crawl_delay\": null,\n",
      "  \"sitemaps\": [],\n",
      "  \"recommended_delay\": 1.0\n",
      "}\n",
      "\n",
      "Sitemap endpoints detected:\n",
      "\n",
      "Total URLs from sitemaps (deduped): 0\n",
      "Candidate URLs to crawl: 0 (showing up to 10)\n",
      "\n",
      "Crawl summary:\n",
      "No pages processed.\n",
      "\n",
      "Analysis summary:\n",
      "{}\n",
      "\n",
      "Report written to: fhb_data/report.md\n"
     ]
    }
   ],
   "source": [
    "# Assignment solution: FHB (Food and Health Bureau) site crawl\n",
    "# Goal: Follow the 7 tasks and produce artifacts under ./fhb_data\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "BASE = \"https://www.fhb.gov.hk\"\n",
    "DATA_DIR = \"fhb_data\"\n",
    "FILES_DIR = os.path.join(DATA_DIR, \"files\")\n",
    "REPORT_PATH = os.path.join(DATA_DIR, \"report.md\")\n",
    "\n",
    "os.makedirs(FILES_DIR, exist_ok=True)\n",
    "\n",
    "session = requests.Session()\n",
    "session.headers.update({\n",
    "    \"User-Agent\": \"GCAP3226-Student-Crawler/1.0 (Educational purpose)\"\n",
    "})\n",
    "\n",
    "# 1) Check robots.txt\n",
    "robots_url = f\"{BASE}/robots.txt\"\n",
    "robots_text = \"\"\n",
    "robots_summary = {}\n",
    "try:\n",
    "    r = session.get(robots_url, timeout=12)\n",
    "    if r.status_code == 200:\n",
    "        robots_text = r.text\n",
    "    else:\n",
    "        robots_text = f\"<HTTP {r.status_code}>\"\n",
    "except Exception as e:\n",
    "    robots_text = f\"<error: {e}>\"\n",
    "\n",
    "# Very simple robots parser\n",
    "rules = {\"user_agents\": {}, \"sitemaps\": []}\n",
    "current_ua = None\n",
    "for raw in robots_text.splitlines():\n",
    "    line = raw.strip()\n",
    "    if not line or line.startswith('#'):\n",
    "        continue\n",
    "    key, _, value = line.partition(':')\n",
    "    key = key.strip().lower()\n",
    "    value = value.strip()\n",
    "    if key == 'user-agent':\n",
    "        current_ua = value\n",
    "        rules[\"user_agents\"].setdefault(current_ua, {\"allow\": [], \"disallow\": [], \"crawl-delay\": None})\n",
    "    elif key in (\"allow\", \"disallow\"):\n",
    "        if current_ua is None:\n",
    "            continue\n",
    "        rules[\"user_agents\"].setdefault(current_ua, {\"allow\": [], \"disallow\": [], \"crawl-delay\": None})\n",
    "        rules[\"user_agents\"][current_ua][key].append(value)\n",
    "    elif key == 'crawl-delay':\n",
    "        if current_ua is None:\n",
    "            continue\n",
    "        try:\n",
    "            delay_val = float(value)\n",
    "        except ValueError:\n",
    "            delay_val = value\n",
    "        rules[\"user_agents\"][current_ua][\"crawl-delay\"] = delay_val\n",
    "    elif key == 'sitemap':\n",
    "        rules[\"sitemaps\"].append(value)\n",
    "\n",
    "ua_all = rules[\"user_agents\"].get('*')\n",
    "recommended_delay = 1.0\n",
    "if ua_all and isinstance(ua_all.get('crawl-delay'), (int, float)):\n",
    "    recommended_delay = max(1.0, float(ua_all['crawl-delay']))\n",
    "\n",
    "robots_summary = {\n",
    "    \"robots_url\": robots_url,\n",
    "    \"ua_star_present\": bool(ua_all),\n",
    "    \"disallow_count\": len([p for p in (ua_all['disallow'] if ua_all else []) if p and p.strip()]),\n",
    "    \"allow_count\": len([p for p in (ua_all['allow'] if ua_all else []) if p and p.strip()]),\n",
    "    \"crawl_delay\": ua_all.get('crawl-delay') if ua_all else None,\n",
    "    \"sitemaps\": rules[\"sitemaps\"],\n",
    "    \"recommended_delay\": recommended_delay,\n",
    "}\n",
    "\n",
    "print(\"Robots.txt summary for FHB:\")\n",
    "print(json.dumps(robots_summary, indent=2, ensure_ascii=False))\n",
    "\n",
    "# 2) Find sitemap(s)\n",
    "sitemap_candidates = [\n",
    "    f\"{BASE}/sitemap.xml\",\n",
    "    f\"{BASE}/sitemap_index.xml\",\n",
    "]\n",
    "\n",
    "found_sitemaps = []\n",
    "for u in set(robots_summary[\"sitemaps\"] + sitemap_candidates):\n",
    "    try:\n",
    "        resp = session.get(u, timeout=12)\n",
    "        if resp.status_code == 200 and resp.text.strip().startswith(\"<?xml\"):\n",
    "            found_sitemaps.append(u)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "print(\"\\nSitemap endpoints detected:\")\n",
    "for sm in found_sitemaps:\n",
    "    print(\" -\", sm)\n",
    "\n",
    "# Helper to parse XML sitemap and return URLs\n",
    "NS = {'ns': 'http://www.sitemaps.org/schemas/sitemap/0.9'}\n",
    "\n",
    "def parse_sitemap(url: str) -> list:\n",
    "    try:\n",
    "        r = session.get(url, timeout=15)\n",
    "        r.raise_for_status()\n",
    "        root = ET.fromstring(r.content)\n",
    "        urls = []\n",
    "        # urlset\n",
    "        for url_elem in root.findall('.//ns:url', NS):\n",
    "            loc = url_elem.find('ns:loc', NS)\n",
    "            if loc is not None and loc.text:\n",
    "                urls.append(loc.text)\n",
    "        # nested sitemap index\n",
    "        for sm in root.findall('.//ns:sitemap', NS):\n",
    "            loc = sm.find('ns:loc', NS)\n",
    "            if loc is not None and loc.text:\n",
    "                urls.extend(parse_sitemap(loc.text))\n",
    "        return list(dict.fromkeys(urls))\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "# 3) Identify target pages likely to have quantitative data\n",
    "keywords = [\n",
    "    \"statistics\", \"data\", \"report\", \"survey\", \"indicator\", \"table\",\n",
    "    \"health\", \"disease\", \"hospital\", \"performance\", \"cases\", \"rate\", \"percentage\"\n",
    "]\n",
    "\n",
    "all_page_urls = []\n",
    "for sm in found_sitemaps[:3]:  # keep it bounded\n",
    "    time.sleep(recommended_delay)\n",
    "    all_page_urls.extend(parse_sitemap(sm))\n",
    "\n",
    "all_page_urls = list(dict.fromkeys(all_page_urls))\n",
    "print(f\"\\nTotal URLs from sitemaps (deduped): {len(all_page_urls)}\")\n",
    "\n",
    "# Filter by keyword in URL to prioritize likely quantitative content\n",
    "candidate_urls = [u for u in all_page_urls if any(k in u.lower() for k in keywords)]\n",
    "# Fallback: if no candidates, sample first N\n",
    "if not candidate_urls:\n",
    "    candidate_urls = all_page_urls[:30]\n",
    "\n",
    "print(f\"Candidate URLs to crawl: {len(candidate_urls)} (showing up to 10)\")\n",
    "for u in candidate_urls[:10]:\n",
    "    print(\" -\", u)\n",
    "\n",
    "# 4) Crawl ethically + 5) Extract data/files\n",
    "number_patterns = [\n",
    "    r\"\\d+(?:\\.\\d+)?%\",           # percentages\n",
    "    r\"\\$\\d+(?:,\\d{3})*\",         # currency\n",
    "    r\"HK\\$\\d+(?:,\\d{3})*\",       # HK$\n",
    "    r\"\\b\\d{1,3}(?:,\\d{3})+\\b\",  # numbers with commas\n",
    "]\n",
    "\n",
    "file_exts = (\".pdf\", \".xlsx\", \".xls\", \".csv\")\n",
    "\n",
    "def detect_quantitative_data(html: bytes, url: str):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    text = soup.get_text(\" \")\n",
    "    meta = {\n",
    "        'url': url,\n",
    "        'number_count': 0,\n",
    "        'table_count': len(soup.find_all('table')),\n",
    "        'keywords': [],\n",
    "        'has_charts': any(w in text.lower() for w in [\"chart\", \"graph\", \"diagram\"]),\n",
    "        'files': [],\n",
    "    }\n",
    "    for pattern in number_patterns:\n",
    "        meta['number_count'] += len(re.findall(pattern, text))\n",
    "    for kw in keywords:\n",
    "        if kw in text.lower():\n",
    "            meta['keywords'].append(kw)\n",
    "    # Find downloadable files\n",
    "    for a in soup.find_all('a', href=True):\n",
    "        href = a['href']\n",
    "        if any(href.lower().endswith(ext) for ext in file_exts):\n",
    "            # Only keep absolute URLs on same domain\n",
    "            if href.startswith('http'):\n",
    "                if urlparse(href).netloc.endswith(urlparse(BASE).netloc.split('.')[-2] + \".gov.hk\") or urlparse(href).netloc.endswith(\"gov.hk\"):\n",
    "                    meta['files'].append(href)\n",
    "    has_data = meta['number_count'] > 5 or meta['table_count'] > 0 or (meta['has_charts'] and meta['keywords'])\n",
    "    return has_data, meta\n",
    "\n",
    "results = []\n",
    "MAX_PAGES = 20\n",
    "for i, u in enumerate(candidate_urls[:MAX_PAGES], 1):\n",
    "    print(f\"[{i}/{min(len(candidate_urls), MAX_PAGES)}] {u}\")\n",
    "    try:\n",
    "        resp = session.get(u, timeout=15)\n",
    "        resp.raise_for_status()\n",
    "        has_data, meta = detect_quantitative_data(resp.content, u)\n",
    "        results.append({\n",
    "            'url': u,\n",
    "            'has_data': has_data,\n",
    "            'numbers': meta['number_count'],\n",
    "            'tables': meta['table_count'],\n",
    "            'keywords': ','.join(meta['keywords'][:5]),\n",
    "            'file_count': len(meta['files'])\n",
    "        })\n",
    "        # Download files (limit to a few to be polite)\n",
    "        for j, furl in enumerate(meta['files'][:2], 1):\n",
    "            try:\n",
    "                time.sleep(recommended_delay)\n",
    "                fr = session.get(furl, timeout=20)\n",
    "                fr.raise_for_status()\n",
    "                fname = os.path.join(FILES_DIR, re.sub(r\"[^A-Za-z0-9_.-]\", \"_\", os.path.basename(urlparse(furl).path)))\n",
    "                with open(fname, 'wb') as fh:\n",
    "                    fh.write(fr.content)\n",
    "                print(f\"    ↳ downloaded: {fname}\")\n",
    "            except Exception as e:\n",
    "                print(f\"    ↳ file download error: {e}\")\n",
    "    except Exception as e:\n",
    "        print(\"  ✗ Error:\", e)\n",
    "    time.sleep(recommended_delay)\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nCrawl summary:\")\n",
    "if not results_df.empty:\n",
    "    print(results_df.head(10).to_string(index=False))\n",
    "    print(f\"Pages with data: {int(results_df['has_data'].sum())} / {len(results_df)}\")\n",
    "else:\n",
    "    print(\"No pages processed.\")\n",
    "\n",
    "# 6) Analyze results: simple aggregates\n",
    "analysis = {}\n",
    "if not results_df.empty:\n",
    "    analysis = {\n",
    "        'pages_scanned': int(len(results_df)),\n",
    "        'pages_with_data': int(results_df['has_data'].sum()),\n",
    "        'avg_numbers': float(results_df['numbers'].mean()),\n",
    "        'avg_tables': float(results_df['tables'].mean()),\n",
    "        'top_keywords': (results_df['keywords']\n",
    "                         .str.split(',')\n",
    "                         .explode()\n",
    "                         .str.strip()\n",
    "                         .replace('', pd.NA)\n",
    "                         .dropna()\n",
    "                         .value_counts()\n",
    "                         .head(10)\n",
    "                         .to_dict()),\n",
    "        'total_files_found': int(results_df['file_count'].sum()),\n",
    "    }\n",
    "\n",
    "print(\"\\nAnalysis summary:\")\n",
    "print(json.dumps(analysis, indent=2, ensure_ascii=False))\n",
    "\n",
    "# 7) Document: write a brief report\n",
    "report_lines = []\n",
    "report_lines.append(\"# FHB Crawl Report\\n\")\n",
    "report_lines.append(f\"Date: {pd.Timestamp.utcnow()} UTC\\n\")\n",
    "report_lines.append(\"\\n## Robots.txt\\n\")\n",
    "report_lines.append(f\"URL: {robots_url}\\n\")\n",
    "if robots_text:\n",
    "    report_lines.append(\"```\\n\" + robots_text + \"\\n```\\n\")\n",
    "else:\n",
    "    report_lines.append(\"(robots.txt not available)\\n\")\n",
    "report_lines.append(\"\\n### Summary\\n\")\n",
    "report_lines.append(json.dumps(robots_summary, indent=2, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "report_lines.append(\"\\n## Sitemaps\\n\")\n",
    "if found_sitemaps:\n",
    "    for sm in found_sitemaps:\n",
    "        report_lines.append(f\"- {sm}\\n\")\n",
    "else:\n",
    "    report_lines.append(\"No sitemap endpoints detected.\\n\")\n",
    "\n",
    "report_lines.append(\"\\n## Crawl Results (sample)\\n\")\n",
    "if not results_df.empty:\n",
    "    report_lines.append(\"```\\n\" + results_df.head(20).to_string(index=False) + \"\\n```\\n\")\n",
    "else:\n",
    "    report_lines.append(\"No pages processed.\\n\")\n",
    "\n",
    "report_lines.append(\"\\n## Analysis\\n\")\n",
    "report_lines.append(json.dumps(analysis, indent=2, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "report_lines.append(\"\\n## Reflection on Ethical Considerations\\n\")\n",
    "report_lines.append(\"- Identified ourselves via a clear User-Agent.\\n\")\n",
    "report_lines.append(\"- Applied at least 1s delay; would increase if server load or crawl-delay specified.\\n\")\n",
    "report_lines.append(\"- Limited to 20 pages and at most 2 file downloads per page.\\n\")\n",
    "report_lines.append(\"- Focused on publicly accessible content and avoided personal data.\\n\")\n",
    "report_lines.append(\"- Preferred sitemaps and conservative filtering; would stop if errors or signs of rate limiting occur.\\n\")\n",
    "\n",
    "with open(REPORT_PATH, 'w', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(report_lines))\n",
    "\n",
    "print(f\"\\nReport written to: {REPORT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7658a67",
   "metadata": {},
   "source": [
    "## Part 7: Ethical Considerations and Best Practices\n",
    "\n",
    "### Legal and Ethical Framework:\n",
    "\n",
    "1. **Copyright:** Respect intellectual property rights\n",
    "2. **Privacy:** Don't collect personal data\n",
    "3. **Terms of Service:** Read and follow website terms\n",
    "4. **Server Load:** Don't overwhelm servers\n",
    "5. **Purpose:** Use data for legitimate research/analysis\n",
    "\n",
    "### Rate Limiting Guidelines:\n",
    "- Small sites: 2-3 seconds between requests\n",
    "- Medium sites: 1-2 seconds\n",
    "- Large sites (e.g., government): 0.5-1 second\n",
    "- **Never** faster than 0.5 seconds\n",
    "\n",
    "### When NOT to Crawl:\n",
    "- robots.txt explicitly disallows\n",
    "- Site requires login\n",
    "- Terms of service prohibit it\n",
    "- Data is available via API\n",
    "- Site is slow or unstable\n",
    "\n",
    "### Alternative: APIs\n",
    "Many government sites offer APIs for data access:\n",
    "- **Hong Kong Open Data:** https://data.gov.hk\n",
    "- Usually better than crawling!\n",
    "- Structured data, legal access, official support"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9708eb4f",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**What You Learned:**\n",
    "1. ✓ How to check robots.txt and respect crawling rules\n",
    "2. ✓ How to discover content using sitemaps\n",
    "3. ✓ How to detect quantitative data in web pages\n",
    "4. ✓ How to implement ethical crawling practices\n",
    "5. ✓ How to compare crawler vs. Google search approaches\n",
    "6. ✓ How to apply these skills to government websites\n",
    "\n",
    "**Next Steps:**\n",
    "- Complete the assignment\n",
    "- Explore Hong Kong Open Data portal\n",
    "- Learn about web scraping libraries (Scrapy, Selenium)\n",
    "- Study data analysis techniques for policy research\n",
    "\n",
    "**Resources:**\n",
    "- [Web Scraping Best Practices](https://www.scrapehero.com/web-scraping-best-practices/)\n",
    "- [Hong Kong Open Data](https://data.gov.hk)\n",
    "- [Beautiful Soup Documentation](https://www.crummy.com/software/BeautifulSoup/)\n",
    "- [Robots.txt Specification](https://www.robotstxt.org/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
